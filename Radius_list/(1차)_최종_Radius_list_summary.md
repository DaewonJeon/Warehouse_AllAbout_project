# Radius_List: 다이소 상권 분석 및 시각화 프로젝트 Summary

> **"카카오 API의 데이터 수집 한계(45개)를 넘어, 서울시 전수 조사를 향한 여정"**
> 버전: v1.0 ~ v2.1

---

## 📋 목차

1. [프로젝트 개요](#1-프로젝트-개요)
2. [기술 스택 총괄](#2-기술-스택-총괄)
3. [버전별 개발 히스토리](#3-버전별-개발-히스토리)
4. [핵심 문제 해결 로그 (Deep Dive)](#4-핵심-문제-해결-로그-deep-dive)
    - [4.1 서울시 모든 다이소 찾기 (지역 쪼개기)](#41-서울시-모든-다이소-찾기-지역-쪼개기)
    - [4.2 PostGIS vs BigQuery (데이터 현실)](#42-postgis-vs-bigquery-데이터-현실)
    - [4.3 반경 45개 제한 돌파 (4분할 전략)](#43-반경-45개-제한-돌파-4분할-전략)
    - [4.4 데이터 무결성 (중복 제거)](#44-데이터-무결성-중복-제거)
5. [아키텍처 요약](#5-아키텍처-요약)
6. [회고 및 결론](#6-회고-및-결론)

---

## 1. 프로젝트 개요

### 1.1 프로젝트 배경
**"다이소 주변에는 정말 스타벅스가 많을까?"**
단순한 호기심에서 시작된 이 프로젝트는, 특정 상권(다이소)을 중심으로 반경 1km 내의 핵심 상권(편의점, 카페 등) 데이터를 수집하고 분석하여 시각화하는 **위치 기반 데이터 파이프라인(Geo-Pipeline)** 구축 프로젝트입니다.

### 1.2 핵심 목표
- **Complete Coverage**: API 제한을 우회하여 서울시 내 **모든** 다이소 지점 확보 (약 260개).
- **High Density Data**: 강남 등 밀집 지역에서도 누락 없는 주변 상권 데이터(편의점/카페) 수집.
- **Visualization**: 텍스트 리스트를 넘어, **지도(Map)** 위에 직관적으로 상권을 표현.

---

## 2. 기술 스택 총괄

| 구분 | Technology | 선택 이유 |
|------|-----------|-----------|
| **Core** | Python, Django 5.2 | 데이터 수집 및 웹 서비스 백엔드 |
| **Database** | **PostgreSQL + PostGIS** | 공간 데이터(Lat/Lng) 저장 및 반경 검색(Spatial Query) 최적화 |
| **API** | **Kakao Local API** | 한국 내 가장 정확한 상권 데이터 보유 (데이터 품질 우수) |
| **Experiment** | Google BigQuery | 대용량 처리 시도 (OSM 데이터 품질 이슈로 기각) |
| **Deploy** | Docker | DB 및 애플리케이션 컨테이너화 |

---

## 3. 버전별 개발 히스토리

### v1.0 - 데이터 수집기 (MVP)
- **기능**: Kakao API를 호출하여 다이소 위치 및 주변 편의점 수집.
- **저장**: SQLite + CSV 파일 저장.
- **한계**: API 호출 속도 제한(Rate Limit) 및 동기식 처리로 인한 속도 저하.

### v1.1 - 안정성 및 정합성 강화
- **개선**: `update_or_create`를 활용한 중복 데이터 방지 로직 도입.
- **이슈**: 대량 수집 시 API의 페이지네이션(Pagination) 한계(최대 45개) 발견.

### v2.0 - 공간 데이터베이스 (PostGIS) & 시각화
- **도입**: 단순 주소 저장을 넘어 **위도(Lat), 경도(Lng)** 좌표 체계 도입.
- **시각화**: Django View를 통해 수집된 데이터를 지도상에 마커로 표시.
- **확장**: PostgreSQL + PostGIS로 마이그레이션하여 공간 쿼리(Distance Calculation) 성능 확보.

### v2.1 - 확장성 실험 (BigQuery vs API)
- **시도**: API 호출 없이 전 세계 데이터를 가진 **Google BigQuery (OpenStreetMap)** 도입 시도.
- **결론**: 데이터 품질 문제(서울 다이소 누락 심각)로 기존 API 방식 고도화로 회귀.

---

## 4. 핵심 문제 해결 로그 (Deep Dive)

이 프로젝트의 가장 큰 도전 과제는 **"외부 API의 제한(Limitations)"**과 **"데이터 품질(Quality)"** 사이의 균형을 맞추는 것이었습니다.

### 4.1 서울시 모든 다이소 찾기 (지역 쪼개기)

**문제(Problem):**
Kakao API에 단순히 "서울 다이소"라고 검색하면, API는 관련도 순으로 최대 45개(15개 × 3페이지)의 결과만 보여줍니다. 하지만 실제 서울시의 다이소는 약 260개입니다. 나머지 200여 개는 검색되지 않는 **데이터 누락**이 발생합니다.

**해결(Solution): "Divide and Conquer (지역 쪼개기)"**
서울시를 하나의 덩어리가 아닌, **25개 자치구(강남구, 서초구, 송파구 ...)**로 쪼개서 검색했습니다.

- "서울 다이소" (X) -> 45개 (실패)
- "강남구 다이소", "마포구 다이소", ... (O) -> 각 구별 10~20개 내외 수집.

> **결과:** 25번의 반복 요청을 통해, API 한계를 넘지 않으면서 **서울시 전체 260여 개 다이소 지점 전수 조사(Full Scan)에 성공**했습니다.

### 4.2 PostGIS vs BigQuery (데이터 현실)

**시도(Trial):**
API 호출의 번거로움과 제한을 피하기 위해, 구글의 **BigQuery (OpenStreetMap Public Dataset)**를 도입해 보았습니다. SQL 쿼리 한 번으로 서울시의 모든 편의점을 가져올 수 있다는 점은 매력적이었습니다.

**좌절(Failure):**
빅쿼리는 도구일 뿐, 핵심은 **데이터(Source Data)**였습니다.
- **PostGIS (Kakao Data)**: 서울시 다이소 **약 260개** (정확함)
- **BigQuery (OSM Data)**: 서울시 다이소 **약 60개** (부정확, 누락 심각)

**결정(Decision):**
> "데이터 분석 프로젝트에서 가장 중요한 것은 **데이터의 품질(Quality)**이다."

도구의 화려함(BigQuery)보다 데이터의 정확도(Kakao API)를 선택했습니다. 대신, 수집한 데이터를 **PostGIS**에 적재함으로써, "정확한 데이터"와 "공간 쿼리의 강력함"을 모두 잡았습니다.

### 4.3 반경 45개 제한 돌파 (4분할 전략)

**문제(Problem):**
다이소 지점을 찾은 후, 반경 1km 내의 편의점을 검색할 때 또다시 **"45개 제한"**에 부딪혔습니다.
강남역 다이소 반경 1km에는 편의점이 100개가 넘지만, API는 거리가 가까운 순으로 **상위 45개**만 주고 나머지는 잘라버립니다(Pagination End).

**해결(Solution): "Spatial Sectoring (공간 분할)"**
1km 반경을 한 번에 요청하지 않고, 중심점을 기준으로 **4분면(Quadrant)**으로 나누어 요청했습니다.

1. **북동쪽 (NE)** 1km 반경
2. **북서쪽 (NW)** 1km 반경
3. **남동쪽 (SE)** 1km 반경
4. **남서쪽 (SW)** 1km 반경

이렇게 영역을 쪼개면 각 영역 내의 편의점 개수가 45개 미만으로 떨어질 확률이 높아져, **누락되었던 46번째~100번째 편의점들도 수집**할 수 있게 되었습니다.

### 4.4 데이터 무결성 (중복 제거)

**문제(Problem):**
"지역 쪼개기(4.1)"와 "4분할 전략(4.3)"을 사용하면, 필연적으로 **중복 수집(Duplicate)**이 발생합니다. 경계선에 걸친 편의점은 A구역 검색에서도 나오고, B구역 검색에서도 나오기 때문입니다.

**해결(Solution): Application Level Upsert**
Application 레벨에서 강력한 중복 제거 로직을 구현했습니다.

1. **Unique Identification**: 상점의 `place_id`(카카오 고유 ID) 또는 `(가게명, 주소)` 조합을 식별자로 사용.
2. **Upsert Logic (`update_or_create`)**:
   - 데이터가 없으면 → **Insert (신규 수집)**
   - 데이터가 이미 있으면 → **Update (정보 갱신, 중복 방지)**

```python
# Core Logic
NearbyStore.objects.update_or_create(
    name=store_name,
    address=address,
    defaults={
        'lat': y,
        'lng': x,
        'category': category_name,
        'base_store': daiso_obj
    }
)
```
> **결과:** 수천 번의 API 호출과 겹치는 검색 영역에도 불구하고, DB에는 **유니크한 가게 데이터**만 깔끔하게 적재되었습니다.

---

## 5. 아키텍처 요약

```
    A[사용자 요청 / Batch] --> B(Django Management Command);
    B --> C{데이터 수집 전략};
    C -- 1. 지역 분할 --> D[Kakao API (District Search)];
    C -- 2. 4분면 분할 --> E[Kakao API (Quadrant Radius)];
    D --> F[Raw Data];
    E --> F;
    F --> G[중복 제거 및 정제 (Upsert)];
    G --> H[(PostgreSQL + PostGIS)];
    H --> I[Django View / API];
    I --> J[Web Map Visualization];
```

1. **Collector**: 지역별/분면별로 API를 잘게 쪼개서 호출 (Coverage 극대화).
2. **Refiner**: Python 로직에서 중복 데이터 제거 및 포맷팅.
3. **Storage**: PostGIS에 위경도(Geometry) 형태로 저장.
4. **Visualizer**: 웹상에서 DB에 저장된 마커들을 시각화.

---

## 6. 회고 및 결론

> **"제약 사항이 창의적인 해결책을 만든다."**

처음에는 API의 45개 제한이 단순한 장애물로만 느껴졌습니다. 하지만 이를 극복하기 위해 **"전체를 부분으로 나누는(Divide and Conquer)"** 알고리즘적 사고를 적용하게 되었고, 이 과정에서 데이터 파이프라인의 견고함이 훨씬 높아졌습니다.

또한, 맹목적인 신기술(BigQuery) 도입보다는 **"현재 내가 가진 데이터의 품질"**을 먼저 검증해야 한다는 중요한 교훈을 얻었습니다.

이 프로젝트는 단순한 크롤러를 넘어, **제한된 자원 내에서 데이터 커버리지(Coverage)와 정합성(Integrity)을 극대화하는 엔지니어링 경험**이었습니다.

---


> **문서 작성**: DH / Z
> **프로젝트 기간**: 2025-11 ~ 2026-01 
